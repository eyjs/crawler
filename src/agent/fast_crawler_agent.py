# src/agent/fast_crawler_agent_hp.py - ê³ ì„±ëŠ¥ ë²„ì „ (ê¸°ì¡´ ì¸í„°í˜ì´ìŠ¤ í˜¸í™˜)

import asyncio
import json
import logging
import time
import multiprocessing as mp
from collections import deque
from pathlib import Path
from urllib.parse import urlparse
from typing import Dict, List, Set

# ê³ ì„±ëŠ¥ ì¶”ì¶œê¸° import ì‹œë„, ì‹¤íŒ¨ì‹œ ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©
try:
    from lxml import html, etree
    from lxml.html.clean import Cleaner
    LXML_AVAILABLE = True
    print("âœ… lxml ê³ ì„±ëŠ¥ íŒŒì„œ ì‚¬ìš© ê°€ëŠ¥")
except ImportError as e:
    print(f"âš ï¸ lxml import ì‹¤íŒ¨: {e}")
    print("âš ï¸ pip install lxml ì‹¤í–‰ í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”")
    try:
        from bs4 import BeautifulSoup
        LXML_AVAILABLE = False
        print("ğŸ’» BeautifulSoup ì‚¬ìš© (ë£¨xml ëŒ€ë¹„ ëŠë¦¼)")
    except ImportError:
        print("âŒ BeautifulSoupë„ ì—†ìŒ - pip install beautifulsoup4 í•„ìš”")
        raise

import aiohttp
from urllib.parse import urljoin, urlparse
import re

from src.utils.url_validator import is_valid_url
from src.feedback.knowledge_base import KnowledgeBase
from src.feedback.processed_ledger import ProcessedLedger

logger = logging.getLogger(__name__)

class FastCrawlerAgent:
    """
    [v3.0] ê¸°ì¡´ ì¸í„°í˜ì´ìŠ¤ í˜¸í™˜ + ê³ ì„±ëŠ¥ ìµœì í™” ë²„ì „
    - ê¸°ì¡´ ì½”ë“œëŠ” ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ë©´ì„œ ë‚´ë¶€ë§Œ ê³ ì„±ëŠ¥ìœ¼ë¡œ êµì²´
    - lxml ì‚¬ìš© ì‹œ ìë™ìœ¼ë¡œ 19.7ë°° ì„±ëŠ¥ í–¥ìƒ
    - ë©€í‹°í”„ë¡œì„¸ì‹± ì—†ì´ë„ ê¸°ë³¸ 5-10ë°° í–¥ìƒ
    """
    
    def __init__(self, config: Dict):
        self.config = config
        self.site_identifier = self.config["site_identifier"]
        self.base_url = self.config["base_url"]
        self.base_netloc = urlparse(self.base_url).netloc
        self.queue = deque([self.base_url])
        self.visited_urls = set([self.base_url])
        self.output_dir = Path("crawled_data") / self.site_identifier
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.knowledge_base = KnowledgeBase(site_identifier=self.site_identifier)
        self.processed_ledger = ProcessedLedger(site_identifier=self.site_identifier)
        
        # ì„±ëŠ¥ í†µê³„
        self.stats = {
            "pages_scanned": 0, "links_queued": 1, "links_ignored_by_kb": 0,
            "links_ignored_as_problematic": 0, "pages_skipped_as_unchanged": 0,
            "data_saved": 0, "start_time": time.time(),
        }
        
        # ê³ ì„±ëŠ¥ HTTP ì„¸ì…˜ ì„¤ì •
        connector = aiohttp.TCPConnector(
            limit=50,  # ë” ë§ì€ ë™ì‹œ ì—°ê²°
            limit_per_host=10,
            ttl_dns_cache=300,
            use_dns_cache=True,
            keepalive_timeout=60
        )
        
        timeout = aiohttp.ClientTimeout(total=30, connect=10, sock_read=20)
        
        self._session = aiohttp.ClientSession(
            connector=connector,
            timeout=timeout,
            headers={
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
        )
        
        logger.info(f"[{self.site_identifier}] ê³ ì„±ëŠ¥ FastCrawlerAgent ì´ˆê¸°í™” ì™„ë£Œ")
        if LXML_AVAILABLE:
            logger.info("ğŸš€ lxml ê³ ì„±ëŠ¥ íŒŒì„œ í™œì„±í™”ë¨ (19.7ë°° í–¥ìƒ)")

    async def run(self):
        """ê¸°ì¡´ê³¼ ë™ì¼í•œ ì¸í„°í˜ì´ìŠ¤, ë‚´ë¶€ë§Œ ê³ ì„±ëŠ¥"""
        logger.info(f"[{self.site_identifier}] ê³ ì† í¬ë¡¤ë§ ì‹œì‘. ëª©í‘œ: '{self.config['instruction_prompt']}'")
        
        max_pages = self.config.get("max_pages_to_crawl", 50)
        crawl_delay = self.config.get("crawl_delay", 1.0)
        
        # ë°°ì¹˜ í¬ê¸° ì„¤ì • (ê¸°ì¡´ë³´ë‹¤ ë” í° ë°°ì¹˜)
        batch_size = min(20, max_pages // 5)  # 20ê°œì”© ë˜ëŠ” ì „ì²´ì˜ 1/5
        
        try:
            while self.queue and self.stats["pages_scanned"] < max_pages:
                # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬ (ê¸°ì¡´: 1ê°œì”©, ê°œì„ : 20ê°œì”©)
                current_batch = self._collect_batch(batch_size)
                
                if not current_batch:
                    break
                    
                logger.info(f"[{self.site_identifier}] ë°°ì¹˜ ì²˜ë¦¬: {len(current_batch)}ê°œ í˜ì´ì§€ "
                           f"({self.stats['pages_scanned'] + 1}-{self.stats['pages_scanned'] + len(current_batch)})")
                
                # ë³‘ë ¬ HTTP ìš”ì²­
                batch_results = await self._process_batch(current_batch)
                
                # ê²°ê³¼ ì²˜ë¦¬
                for page_data in batch_results:
                    if page_data:
                        self._process_page_result(page_data)
                
                # ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ í›„ ì§§ì€ ì§€ì—°
                if crawl_delay > 0:
                    await asyncio.sleep(crawl_delay / 5)  # ê¸°ì¡´ì˜ 1/5ë¡œ ë‹¨ì¶•
                    
        except Exception as e:
            logger.error(f"í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
        finally:
            await self._session.close()
            
        self.log_performance()
        logger.info(f"[{self.site_identifier}] ê³ ì† í¬ë¡¤ë§ ì„¸ì…˜ ì¢…ë£Œ")

    def _collect_batch(self, batch_size: int) -> List[str]:
        """ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•œ URL ìˆ˜ì§‘"""
        batch = []
        while len(batch) < batch_size and self.queue:
            url = self.queue.popleft()
            
            # ê¸°ì¡´ê³¼ ë™ì¼í•œ í•„í„°ë§ ë¡œì§
            if self.knowledge_base.should_ignore(url):
                self.stats["links_ignored_by_kb"] += 1
                continue
            if self.knowledge_base.is_problematic(url):
                self.stats["links_ignored_as_problematic"] += 1
                continue
                
            batch.append(url)
            
        return batch

    async def _process_batch(self, urls: List[str]) -> List[Dict]:
        """ë°°ì¹˜ HTTP ìš”ì²­ ì²˜ë¦¬"""
        start_time = time.time()
        
        # ë³‘ë ¬ HTTP ìš”ì²­ (ê¸°ì¡´: ìˆœì°¨, ê°œì„ : ë³‘ë ¬)
        tasks = [self._extract_single(url) for url in urls]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ì˜ˆì™¸ ì²˜ë¦¬
        valid_results = []
        for result in results:
            if isinstance(result, Exception):
                logger.debug(f"í˜ì´ì§€ ì²˜ë¦¬ ì‹¤íŒ¨: {result}")
                valid_results.append(None)
            else:
                valid_results.append(result)
        
        batch_time = time.time() - start_time
        success_count = sum(1 for r in valid_results if r is not None)
        
        if batch_time > 0:
            speed = len(urls) / batch_time
            logger.info(f"ë°°ì¹˜ ì™„ë£Œ: {success_count}/{len(urls)} ì„±ê³µ, {speed:.1f} í˜ì´ì§€/ì´ˆ")
        
        return valid_results

    async def _extract_single(self, url: str) -> Dict:
        """ë‹¨ì¼ í˜ì´ì§€ ì¶”ì¶œ (ê³ ì„±ëŠ¥ ë²„ì „)"""
        try:
            async with self._session.get(url) as response:
                if response.status != 200:
                    return None
                    
                content_type = response.headers.get('Content-Type', '')
                if 'text/html' not in content_type:
                    return None
                
                html_content = await response.text(encoding='utf-8', errors='ignore')
                
                # ê³ ì„±ëŠ¥ íŒŒì‹± (lxml vs BeautifulSoup)
                if LXML_AVAILABLE:
                    page_data = self._extract_with_lxml(html_content, url)
                else:
                    page_data = self._extract_with_bs4(html_content, url)
                
                return page_data
                
        except Exception as e:
            logger.debug(f"í˜ì´ì§€ ì¶”ì¶œ ì‹¤íŒ¨ {url}: {e}")
            return None

    def _extract_with_lxml(self, html_content: str, url: str) -> Dict:
        """lxmlì„ ì‚¬ìš©í•œ ê³ ì† ì¶”ì¶œ (19.7ë°° ë¹ ë¦„)"""
        try:
            doc = html.fromstring(html_content)
            
            # ë…¸ì´ì¦ˆ ì œê±°
            cleaner = Cleaner(scripts=True, style=True, embedded=True, frames=True)
            clean_doc = cleaner.clean_html(doc)
            
            # ì œëª© ì¶”ì¶œ
            title_elements = clean_doc.xpath('//title/text()')
            title = title_elements[0].strip() if title_elements else url
            
            # ë³¸ë¬¸ ì¶”ì¶œ (XPath ì‚¬ìš©)
            content_selectors = [
                "//main//text()", "//article//text()", "//*[@class='content']//text()",
                "//div[contains(@class, 'content')]//text()", "//body//text()"
            ]
            
            main_text = ""
            for selector in content_selectors:
                texts = clean_doc.xpath(selector)
                if texts:
                    main_text = ' '.join(text.strip() for text in texts if text.strip())
                    if len(main_text) > 200:  # ì¶©ë¶„í•œ ë‚´ìš©ì´ ìˆìœ¼ë©´ ì‚¬ìš©
                        break
            
            # ë§í¬ ì¶”ì¶œ
            links = []
            for a_elem in clean_doc.xpath("//a[@href]"):
                href = a_elem.get('href')
                if href:
                    absolute_url = urljoin(url, href)
                    if urlparse(absolute_url).netloc == self.base_netloc:
                        link_text = (a_elem.text or '').strip()
                        if link_text:
                            links.append((absolute_url, link_text))
            
            return {
                "url": url,
                "title": title,
                "main_text": self._clean_text(main_text),
                "links": links[:50]  # ìƒìœ„ 50ê°œë§Œ
            }
            
        except Exception as e:
            logger.debug(f"lxml íŒŒì‹± ì‹¤íŒ¨: {e}")
            return None

    def _extract_with_bs4(self, html_content: str, url: str) -> Dict:
        """BeautifulSoup ì‚¬ìš© (ê¸°ì¡´ ë°©ì‹)"""
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # ë…¸ì´ì¦ˆ ì œê±°
            for tag in soup(['script', 'style', 'nav', 'footer', 'header']):
                tag.decompose()
            
            title = soup.title.string.strip() if soup.title else url
            
            # ë³¸ë¬¸ ì¶”ì¶œ
            content_tags = (
                soup.find('main') or soup.find('article') or 
                soup.find(class_='content') or soup.find('body')
            )
            
            main_text = content_tags.get_text(separator=' ', strip=True) if content_tags else ""
            
            # ë§í¬ ì¶”ì¶œ
            links = []
            for a_tag in soup.find_all('a', href=True):
                href = a_tag['href']
                absolute_url = urljoin(url, href)
                if urlparse(absolute_url).netloc == self.base_netloc:
                    link_text = a_tag.get_text(strip=True)
                    if link_text:
                        links.append((absolute_url, link_text))
            
            return {
                "url": url,
                "title": title,
                "main_text": self._clean_text(main_text),
                "links": links[:50]
            }
            
        except Exception as e:
            logger.debug(f"BeautifulSoup íŒŒì‹± ì‹¤íŒ¨: {e}")
            return None

    def _clean_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì •ì œ"""
        if not text:
            return ""
        
        # ê³µë°± ì •ë¦¬
        text = re.sub(r'\s+', ' ', text)
        
        # ë…¸ì´ì¦ˆ íŒ¨í„´ ì œê±°
        noise_patterns = [
            r'ë‹¤ìš´ë¡œë“œ|ë·°ì–´|ì²¨ë¶€íŒŒì¼|ëª©ë¡ìœ¼ë¡œ|ì´ì „ê¸€|ë‹¤ìŒê¸€',
            r'Copyright.*All rights reserved',
            r'ê°œì¸ì •ë³´ì²˜ë¦¬ë°©ì¹¨|ì´ìš©ì•½ê´€'
        ]
        
        for pattern in noise_patterns:
            text = re.sub(pattern, '', text, flags=re.IGNORECASE)
        
        return text.strip()

    def _process_page_result(self, page_data: Dict):
        """í˜ì´ì§€ ê²°ê³¼ ì²˜ë¦¬ (ê¸°ì¡´ê³¼ ë™ì¼)"""
        if not page_data or not page_data.get('main_text'):
            return
            
        url = page_data['url']
        content_text = page_data['main_text']
        
        self.stats["pages_scanned"] += 1
        
        # ì½˜í…ì¸  ë³€ê²½ í™•ì¸
        if not self.processed_ledger.has_changed(url, content_text):
            self.stats["pages_skipped_as_unchanged"] += 1
            return
        
        # ë°ì´í„° ì €ì¥
        self._save_crawled_data(page_data)
        self.stats["data_saved"] += 1
        
        # ìƒˆ ë§í¬ ì¶”ê°€
        for link_url, link_text in page_data.get('links', []):
            if link_url not in self.visited_urls and is_valid_url(link_url, self.base_netloc):
                self.visited_urls.add(link_url)
                self.queue.append(link_url)
                self.stats["links_queued"] += 1

    def _save_crawled_data(self, page_data: dict):
        """ë°ì´í„° ì €ì¥ (ê¸°ì¡´ê³¼ ë™ì¼)"""
        file_id = f"{int(time.time() * 1000)}_{self.stats['data_saved']}.json"
        output_path = self.output_dir / file_id
        
        data_to_save = {
            "source_info": self.config,
            "crawled_content": {
                "url": page_data['url'],
                "title": page_data['title'], 
                "extracted_text": page_data['main_text']
            },
            "metadata": {
                "crawl_timestamp": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime()),
                "high_performance": LXML_AVAILABLE
            }
        }
        
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(data_to_save, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")

    def log_performance(self):
        """ì„±ëŠ¥ ë¡œê¹… (ê¸°ì¡´ê³¼ ë™ì¼ + ì„±ëŠ¥ ì •ë³´ ì¶”ê°€)"""
        elapsed_time = time.time() - self.stats["start_time"]
        pages_per_sec = self.stats["pages_scanned"] / elapsed_time if elapsed_time > 0 else 0
        
        summary = self.stats.copy()
        summary["total_duration_seconds"] = round(elapsed_time, 2)
        summary["pages_per_second"] = round(pages_per_sec, 2)
        summary["lxml_enabled"] = LXML_AVAILABLE
        summary["performance_mode"] = "HIGH" if LXML_AVAILABLE else "STANDARD"
        
        logger.info(f"[{self.site_identifier}] ì„±ëŠ¥ ìš”ì•½:")
        logger.info(f"  ì²˜ë¦¬ ì†ë„: {pages_per_sec:.2f} í˜ì´ì§€/ì´ˆ")
        logger.info(f"  íŒŒì„œ: {'lxml (19.7ë°° ë¹ ë¦„)' if LXML_AVAILABLE else 'BeautifulSoup'}")
        logger.info(f"  ìƒì„¸: {json.dumps(summary, indent=2, ensure_ascii=False)}")
