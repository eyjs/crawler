import asyncio
import json
import logging
from pathlib import Path
import time
import aiofiles
import re

from src.llm.llm_client import LlmClient
from src.packet.data_packet import create_data_packet
from src.feedback.knowledge_base import KnowledgeBase
from src.feedback.processed_ledger import ProcessedLedger
from config.settings import config

logger = logging.getLogger(__name__)

class LlmProcessingWorker:
    """
    [v2.0] '3ë‹¨ê³„ ê²€ì¦' (í’ˆì§ˆ ê²€ì‚¬ -> ê´€ë ¨ì„± ê²€ì‚¬ -> ì‹¬ì¸µ ë¶„ì„) ë¡œì§ì„ ì ìš©í•˜ì—¬
    ë°ì´í„° ì²˜ë¦¬ì˜ ì •í™•ì„±ê³¼ íš¨ìœ¨ì„±ì„ ê·¹ëŒ€í™”í•œ ìµœì¢… ì›Œì»¤.
    """
    def __init__(self, site_identifier: str):
        self.site_identifier = site_identifier
        self.relevance_threshold = config.relevance_threshold
        self.input_dir = Path("crawled_data") / self.site_identifier
        self.processed_dir = self.input_dir / "processed"
        self.rejected_dir = self.input_dir / "rejected"
        self.output_dir = Path("output_packets") / self.site_identifier
        for d in [self.input_dir, self.processed_dir, self.rejected_dir, self.output_dir]:
            d.mkdir(parents=True, exist_ok=True)
        self.llm_client = LlmClient()
        self.knowledge_base = KnowledgeBase(site_identifier=self.site_identifier)
        self.processed_ledger = ProcessedLedger(site_identifier=self.site_identifier)
        self.stats = {"processed": 0, "accepted": 0, "rejected": 0, "parsing_failures": 0, "quality_rejected": 0, "gatekeeper_rejected": 0}
        logger.info(f"[{self.site_identifier}] LlmProcessingWorker ì´ˆê¸°í™” ì™„ë£Œ.")

    def _is_low_quality_text(self, text: str) -> bool:
        """ [ì‹ ê·œ] LLM í˜¸ì¶œ ì „, í…ìŠ¤íŠ¸ê°€ ëª©ë¡í˜• ë°ì´í„°ì²˜ëŸ¼ ë³´ì´ëŠ”ì§€ í”„ë¡œê·¸ë˜ë° ë°©ì‹ìœ¼ë¡œ ê²€ì‚¬í•©ë‹ˆë‹¤. """
        lines = text.split('\n')
        if len(lines) < 5: return False # ë„ˆë¬´ ì§§ì€ í…ìŠ¤íŠ¸ëŠ” íŒë‹¨ ë³´ë¥˜

        short_line_count = 0
        date_pattern = r'\d{4}-\d{2}-\d{2}' # YYYY-MM-DD í˜•ì‹ì˜ ë‚ ì§œ íŒ¨í„´

        for line in lines:
            if len(line.strip()) < 50: # 50ì ë¯¸ë§Œì˜ ì§§ì€ ì¤„
                short_line_count += 1
            # ë‚ ì§œ íŒ¨í„´ì´ í¬í•¨ëœ ì¤„ë„ ëª©ë¡ì˜ íŠ¹ì§•ìœ¼ë¡œ ê°„ì£¼
            if re.search(date_pattern, line):
                short_line_count += 0.5 # ê°€ì¤‘ì¹˜ ë¶€ì—¬

        # ì „ì²´ ì¤„ì˜ 70% ì´ìƒì´ ì§§ì€ ì¤„ì´ê±°ë‚˜ ë‚ ì§œë¥¼ í¬í•¨í•˜ë©´ ì €í’ˆì§ˆ(ëª©ë¡)ë¡œ íŒë‹¨
        if (short_line_count / len(lines)) > 0.7:
            return True

        return False

    async def run(self):
        logger.info(f"[{self.site_identifier}] LLM ì›Œì»¤ê°€ ì‹¤ì‹œê°„ ê°ì‹œë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")
        try:
            while True:
                processed_in_cycle = await self._scan_and_process_once()
                if processed_in_cycle == 0: await asyncio.sleep(15)
                else: await asyncio.sleep(2)
        except asyncio.CancelledError:
             logger.info(f"[{self.site_identifier}] ì›Œì»¤ê°€ ì¢…ë£Œë©ë‹ˆë‹¤.")
        finally:
            logger.info(f"[{self.site_identifier}] ìµœì¢… í†µê³„: {json.dumps(self.stats, indent=2, ensure_ascii=False)}")

    async def _scan_and_process_once(self) -> int:
        files = [f for f in self.input_dir.iterdir() if f.is_file() and f.suffix == '.json']
        if not files: return 0
        logger.info(f"[{self.site_identifier}] {len(files)}ê°œì˜ ìƒˆ íŒŒì¼ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
        tasks = [self._process_file(f) for f in files]
        await asyncio.gather(*tasks)
        return len(files)

    async def _process_file(self, file_path: Path):
        """ [ìˆ˜ì •ë¨] 3ë‹¨ê³„ ê²€ì¦ ë¡œì§ì„ ì‚¬ìš©í•˜ì—¬ íŒŒì¼ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤. """
        try:
            async with aiofiles.open(file_path, 'r', encoding='utf-8') as f:
                raw_data = json.loads(await f.read())
            url = raw_data['crawled_content']['url']
            text = raw_data['crawled_content']['extracted_text']
            prompt = raw_data['source_info']['instruction_prompt']

            # --- ê²€ì¦ 0ë‹¨ê³„: íŒŒì‹± ì‹¤íŒ¨ ì—¬ë¶€ íŒë‹¨ ---
            if "--- ì²¨ë¶€ íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨:" in text:
                self.knowledge_base.update_failure(url)
                self.stats["parsing_failures"] += 1
                await self._reject_and_archive(file_path, reason="ì²¨ë¶€ íŒŒì¼ íŒŒì‹± ì‹¤íŒ¨")
                return

            # --- [ì‹ ê·œ] ê²€ì¦ 1ë‹¨ê³„: í”„ë¡œê·¸ë˜ë° ë°©ì‹ í’ˆì§ˆ ê²€ì‚¬ ---
            if self._is_low_quality_text(text):
                logger.warning(f"ğŸ“‰ í’ˆì§ˆ ê²€ì‚¬: ëª©ë¡í˜• ë°ì´í„°ë¡œ ì¶”ì •ë˜ì–´ íê¸° - {url}")
                self.knowledge_base.update_score(url, 0.0) # ì €í’ˆì§ˆì´ë¯€ë¡œ 0ì  í•™ìŠµ
                self.stats["quality_rejected"] += 1
                await self._reject_and_archive(file_path, "í”„ë¡œê·¸ë¨ì— ì˜í•´ ì €í’ˆì§ˆ(ëª©ë¡í˜•)ìœ¼ë¡œ ë¶„ë¥˜ë¨")
                return

            # --- ê²€ì¦ 2ë‹¨ê³„: LLM ê²Œì´íŠ¸í‚¤í¼ë¥¼ í†µí•œ ë¹ ë¥¸ ê´€ë ¨ì„± ê²€ì‚¬ ---
            if not await self.llm_client.is_content_relevant(text, prompt):
                logger.info(f"ğŸš« LLM ê²Œì´íŠ¸í‚¤í¼: ê´€ë ¨ ì—†ëŠ” ì½˜í…ì¸ ë¡œ íŒë‹¨í•˜ì—¬ íê¸° - {url}")
                self.knowledge_base.update_score(url, 0.0)
                self.stats["gatekeeper_rejected"] += 1
                await self._reject_and_archive(file_path, "LLMì— ì˜í•´ ê´€ë ¨ì„± ì—†ìŒìœ¼ë¡œ ë¶„ë¥˜ë¨")
                return

            # --- ê²€ì¦ 3ë‹¨ê³„: ê´€ë ¨ì„± ìˆëŠ” ì½˜í…ì¸ ì— ëŒ€í•´ì„œë§Œ ì‹¬ì¸µ ë¶„ì„ ìˆ˜í–‰ ---
            logger.info(f"âœ… LLM ê²Œì´íŠ¸í‚¤í¼ í†µê³¼: ì‹¬ì¸µ ë¶„ì„ ì‹œì‘ - {url}")
            llm_result = await self.llm_client.evaluate_and_enhance_content(text, prompt)
            score = llm_result.get('relevance_score', 0.0)
            self.knowledge_base.update_score(url, score)

            if score >= self.relevance_threshold:
                await self._accept_and_package(raw_data, llm_result, file_path)
            else:
                await self._reject_and_archive(file_path, f"ë‚®ì€ ê´€ë ¨ì„± ì ìˆ˜: {score:.2f}")

        except Exception as e:
            logger.error(f"íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {file_path}. ì›ì¸: {e}", exc_info=True)
            if 'url' in locals(): self.knowledge_base.update_failure(url)
            await self._reject_and_archive(file_path, f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    async def _accept_and_package(self, raw_data: dict, llm_result: dict, original_path: Path):
        final_packet = create_data_packet(
            agent_id="llm-worker-01", config=raw_data['source_info'], page_data=raw_data['crawled_content'],
            extracted_text=raw_data['crawled_content']['extracted_text'],
            relevance_score=llm_result['relevance_score'], enhanced_data=llm_result
        )
        packet_filename = f"{int(time.time() * 1000)}_{original_path.stem}.json"
        output_filepath = self.output_dir / packet_filename
        try:
            async with aiofiles.open(output_filepath, 'w', encoding='utf-8') as f:
                await f.write(json.dumps(final_packet, indent=2, ensure_ascii=False))
            logger.info(f"âœ… íŒ¨í‚· ì €ì¥ ì™„ë£Œ (ì ìˆ˜: {llm_result['relevance_score']:.2f}): {output_filepath.name}")
        except Exception as e:
            logger.error(f"íŒ¨í‚· ì €ì¥ ì‹¤íŒ¨: {e}")

        self.processed_ledger.add_processed_item(raw_data['crawled_content']['url'], raw_data['crawled_content']['extracted_text'])
        self.stats["processed"] += 1
        self.stats["accepted"] += 1
        await self._move_file(original_path, self.processed_dir)

    async def _reject_and_archive(self, file_path: Path, reason: str):
        logger.warning(f"ì½˜í…ì¸  íê¸°: {file_path.name}. ì›ì¸: {reason}")
        self.stats["processed"] += 1
        self.stats["rejected"] += 1
        await self._move_file(file_path, self.rejected_dir)

    async def _move_file(self, src: Path, dest_dir: Path):
        try:
            src.rename(dest_dir / src.name)
        except Exception as e:
            logger.error(f"íŒŒì¼ ì´ë™ ì‹¤íŒ¨: {src} -> {dest_dir / src.name}, ì›ì¸: {e}")

