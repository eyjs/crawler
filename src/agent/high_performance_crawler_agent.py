# src/agent/high_performance_crawler_agent.py

import asyncio
import json
import logging
import time
import multiprocessing as mp
from collections import deque
from pathlib import Path
from urllib.parse import urlparse
from typing import Dict, List, Set
from dataclasses import dataclass, asdict

from src.crawler.high_performance_extractor import FastDataExtractor, BatchExtractionConfig
from src.utils.url_validator import is_valid_url
from src.feedback.knowledge_base import KnowledgeBase
from src.feedback.processed_ledger import ProcessedLedger

logger = logging.getLogger(__name__)

@dataclass
class CrawlingStats:
    """í¬ë¡¤ë§ í†µê³„ë¥¼ ë‹´ëŠ” ë°ì´í„° í´ë˜ìŠ¤"""
    pages_discovered: int = 0
    pages_processed: int = 0
    pages_succeeded: int = 0
    pages_failed: int = 0
    pages_skipped_unchanged: int = 0
    pages_skipped_by_kb: int = 0
    pages_skipped_problematic: int = 0
    total_links_found: int = 0
    processing_time: float = 0.0
    avg_speed: float = 0.0
    
    def success_rate(self) -> float:
        return (self.pages_succeeded / max(1, self.pages_processed)) * 100
    
    def to_dict(self) -> dict:
        return asdict(self)

class HighPerformanceCrawlerAgent:
    """
    [v3.0] ëŒ€ê·œëª¨ í¬ë¡¤ë§ì„ ìœ„í•œ ê³ ì„±ëŠ¥ ì—ì´ì „íŠ¸
    
    ì£¼ìš” íŠ¹ì§•:
    - 2000í˜ì´ì§€ ì´ìƒ ì²˜ë¦¬ ê°€ëŠ¥
    - ë©€í‹°í”„ë¡œì„¸ì‹± + ë°°ì¹˜ ì²˜ë¦¬
    - ì§€ëŠ¥í˜• í ê´€ë¦¬
    - ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
    - ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì²˜ë¦¬
    """
    
    def __init__(self, config: Dict):
        self.config = config
        self.site_identifier = self.config["site_identifier"]
        self.base_url = self.config["base_url"]
        self.base_netloc = urlparse(self.base_url).netloc
        
        # íì™€ ì§‘í•©ì„ íš¨ìœ¨ì ìœ¼ë¡œ ê´€ë¦¬
        self.pending_urls = deque([self.base_url])
        self.visited_urls: Set[str] = {self.base_url}
        self.failed_urls: Set[str] = set()
        
        # ë””ë ‰í† ë¦¬ ì„¤ì •
        self.output_dir = Path("crawled_data") / self.site_identifier
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # ì§€ì‹ ë² ì´ìŠ¤ì™€ ì²˜ë¦¬ ê¸°ë¡
        self.knowledge_base = KnowledgeBase(site_identifier=self.site_identifier)
        self.processed_ledger = ProcessedLedger(site_identifier=self.site_identifier)
        
        # í†µê³„ ì¶”ì 
        self.stats = CrawlingStats()
        
        # ê³ ì„±ëŠ¥ ì¶”ì¶œê¸° ì„¤ì •
        max_pages = self.config.get("max_pages_to_crawl", 2000)
        cpu_count = mp.cpu_count()
        
        extractor_config = BatchExtractionConfig(
            batch_size=min(100, max_pages // 10),  # ì „ì²´ì˜ 10% ë˜ëŠ” ìµœëŒ€ 100
            max_workers=min(cpu_count, 12),        # CPU ì½”ì–´ ìˆ˜ ë˜ëŠ” ìµœëŒ€ 12
            chunk_size=20,                         # ì²­í¬ í¬ê¸°
            timeout=30                             # íƒ€ì„ì•„ì›ƒ
        )
        
        self.extractor = FastDataExtractor()
        self.extractor.hp_extractor.config = extractor_config
        
        logger.info(f"[{self.site_identifier}] ê³ ì„±ëŠ¥ í¬ë¡¤ëŸ¬ ì—ì´ì „íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"ìµœëŒ€ í˜ì´ì§€: {max_pages}, ì›Œì»¤: {extractor_config.max_workers}, "
                   f"ë°°ì¹˜ í¬ê¸°: {extractor_config.batch_size}")

    async def run(self):
        """ë©”ì¸ í¬ë¡¤ë§ ë£¨í”„ - ëŒ€ê·œëª¨ ì²˜ë¦¬ ìµœì í™”"""
        logger.info(f"[{self.site_identifier}] ê³ ì„±ëŠ¥ í¬ë¡¤ë§ ì‹œì‘")
        logger.info(f"ëª©í‘œ: '{self.config['instruction_prompt']}'")
        
        start_time = time.time()
        max_pages = self.config.get("max_pages_to_crawl", 2000)
        crawl_delay = self.config.get("crawl_delay", 0.5)  # ê³ ì† ì²˜ë¦¬ë¥¼ ìœ„í•´ ë‹¨ì¶•
        
        try:
            while self.pending_urls and self.stats.pages_processed < max_pages:
                # ë°°ì¹˜ í¬ê¸°ë§Œí¼ URL ìˆ˜ì§‘
                current_batch = self._collect_batch_urls()
                if not current_batch:
                    logger.info("ë” ì´ìƒ ì²˜ë¦¬í•  URLì´ ì—†ìŠµë‹ˆë‹¤.")
                    break
                
                logger.info(f"[{self.site_identifier}] ë°°ì¹˜ ì²˜ë¦¬ ì¤‘: "
                           f"{len(current_batch)}ê°œ URL ({self.stats.pages_processed + 1}-"
                           f"{self.stats.pages_processed + len(current_batch)})")
                
                # ë°°ì¹˜ ì²˜ë¦¬ ì‹¤í–‰
                batch_results = await self._process_batch(current_batch)
                
                # ê²°ê³¼ ì²˜ë¦¬ ë° ìƒˆ ë§í¬ ìˆ˜ì§‘
                await self._process_batch_results(batch_results)
                
                # ì§„í–‰ ìƒí™© ë¡œê¹…
                self._log_progress()
                
                # ì ì‘í˜• ì§€ì—° ì‹œê°„ (ë¶€í•˜ ì¡°ì ˆ)
                if crawl_delay > 0:
                    await asyncio.sleep(crawl_delay * len(current_batch) / 10)
                
        except Exception as e:
            logger.error(f"[{self.site_identifier}] í¬ë¡¤ë§ ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜: {e}", exc_info=True)
        
        finally:
            # ë¦¬ì†ŒìŠ¤ ì •ë¦¬
            await self.extractor.close_session()
            
            # ìµœì¢… í†µê³„
            self.stats.processing_time = time.time() - start_time
            if self.stats.processing_time > 0:
                self.stats.avg_speed = self.stats.pages_processed / self.stats.processing_time
            
            self._log_final_statistics()
            
        logger.info(f"[{self.site_identifier}] ê³ ì„±ëŠ¥ í¬ë¡¤ë§ ì™„ë£Œ")

    def _collect_batch_urls(self) -> List[str]:
        """ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•œ URL ìˆ˜ì§‘"""
        batch_size = self.extractor.hp_extractor.config.batch_size
        current_batch = []
        
        # íì—ì„œ ë°°ì¹˜ í¬ê¸°ë§Œí¼ URL ìˆ˜ì§‘
        while len(current_batch) < batch_size and self.pending_urls:
            url = self.pending_urls.popleft()
            
            # ì§€ì‹ ë² ì´ìŠ¤ ê¸°ë°˜ ì‚¬ì „ í•„í„°ë§
            if self.knowledge_base.should_ignore(url):
                self.stats.pages_skipped_by_kb += 1
                continue
                
            if self.knowledge_base.is_problematic(url):
                self.stats.pages_skipped_problematic += 1
                continue
            
            current_batch.append(url)
        
        return current_batch

    async def _process_batch(self, urls: List[str]) -> List[Dict]:
        """URL ë°°ì¹˜ë¥¼ ë³‘ë ¬ ì²˜ë¦¬"""
        batch_start_time = time.time()
        
        # ê³ ì„±ëŠ¥ ë°°ì¹˜ ì¶”ì¶œ ì‹¤í–‰
        results = await self.extractor.extract_batch_optimized(
            urls, self.base_url, self.site_identifier
        )
        
        # í†µê³„ ì—…ë°ì´íŠ¸
        self.stats.pages_processed += len(urls)
        successful_results = [r for r in results if r is not None]
        self.stats.pages_succeeded += len(successful_results)
        self.stats.pages_failed += len(urls) - len(successful_results)
        
        batch_time = time.time() - batch_start_time
        batch_speed = len(urls) / batch_time if batch_time > 0 else 0
        
        logger.info(f"ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ: {len(successful_results)}/{len(urls)} ì„±ê³µ, "
                   f"{batch_speed:.1f} í˜ì´ì§€/ì´ˆ")
        
        return successful_results

    async def _process_batch_results(self, batch_results: List[Dict]):
        """ë°°ì¹˜ ê²°ê³¼ ì²˜ë¦¬ ë° ìƒˆ ë§í¬ ë°œê²¬"""
        new_links_count = 0
        saved_count = 0
        
        for page_data in batch_results:
            if not page_data or not page_data.get('main_text'):
                continue
            
            url = page_data['url']
            content_text = page_data['main_text']
            
            # ì½˜í…ì¸  ë³€ê²½ í™•ì¸
            if not self.processed_ledger.has_changed(url, content_text):
                self.stats.pages_skipped_unchanged += 1
                logger.debug(f"ì½˜í…ì¸  ë³€ê²½ ì—†ìŒ, ì €ì¥ ê±´ë„ˆë›°: {url}")
                continue
            
            # ë°ì´í„° ì €ì¥
            self._save_crawled_data(page_data)
            self.processed_ledger.add_processed_item(url, content_text)
            saved_count += 1
            
            # ìƒˆ ë§í¬ ë°œê²¬ ë° í ì¶”ê°€
            links = page_data.get('links', [])
            self.stats.total_links_found += len(links)
            
            for link_url, link_text in links:
                if self._should_add_to_queue(link_url):
                    self.pending_urls.append(link_url)
                    self.visited_urls.add(link_url)
                    new_links_count += 1
        
        logger.info(f"ë°°ì¹˜ ê²°ê³¼ ì²˜ë¦¬: {saved_count}ê°œ ì €ì¥, {new_links_count}ê°œ ìƒˆ ë§í¬ ë°œê²¬")
        self.stats.pages_discovered += new_links_count

    def _should_add_to_queue(self, url: str) -> bool:
        """URLì„ íì— ì¶”ê°€í• ì§€ íŒë‹¨"""
        if url in self.visited_urls or url in self.failed_urls:
            return False
            
        if not is_valid_url(url, self.base_netloc):
            return False
            
        # í í¬ê¸° ì œí•œ (ë©”ëª¨ë¦¬ ë³´í˜¸)
        if len(self.pending_urls) > 10000:  # ìµœëŒ€ 10,000ê°œ URLê¹Œì§€ íì— ë³´ê´€
            return False
            
        return True

    def _save_crawled_data(self, page_data: dict):
        """í¬ë¡¤ë§ ë°ì´í„° ì €ì¥ (ê³ ì„±ëŠ¥ ë²„ì „)"""
        timestamp = int(time.time() * 1000)
        file_id = f"{timestamp}_{self.stats.pages_succeeded}.json"
        output_path = self.output_dir / file_id
        
        data_to_save = {
            "source_info": self.config,
            "crawled_content": {
                "url": page_data['url'],
                "title": page_data['title'],
                "extracted_text": page_data['main_text']
            },
            "metadata": {
                "crawl_timestamp": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime()),
                "processing_order": self.stats.pages_succeeded,
                "batch_processed": True
            }
        }
        
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(data_to_save, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"íŒŒì¼ ì €ì¥ ì‹¤íŒ¨ {output_path}: {e}")

    def _log_progress(self):
        """ì§„í–‰ ìƒí™© ë¡œê¹…"""
        if self.stats.pages_processed % 100 == 0:  # 100ê°œë§ˆë‹¤ ë¡œê¹…
            max_pages = self.config.get("max_pages_to_crawl", 2000)
            progress_pct = (self.stats.pages_processed / max_pages) * 100
            
            logger.info(f"[{self.site_identifier}] ì§„í–‰ë¥ : {progress_pct:.1f}% "
                       f"({self.stats.pages_processed}/{max_pages})")
            logger.info(f"ì„±ê³µë¥ : {self.stats.success_rate():.1f}%, "
                       f"í í¬ê¸°: {len(self.pending_urls)}, "
                       f"í‰ê·  ì†ë„: {self.stats.avg_speed:.1f} í˜ì´ì§€/ì´ˆ")

    def _log_final_statistics(self):
        """ìµœì¢… í†µê³„ ë¡œê¹…"""
        stats_dict = self.stats.to_dict()
        
        logger.info(f"[{self.site_identifier}] ìµœì¢… í¬ë¡¤ë§ í†µê³„:")
        logger.info(f"  ì´ ì²˜ë¦¬ ì‹œê°„: {self.stats.processing_time:.2f}ì´ˆ")
        logger.info(f"  ì²˜ë¦¬ëœ í˜ì´ì§€: {self.stats.pages_processed}ê°œ")
        logger.info(f"  ì„±ê³µí•œ í˜ì´ì§€: {self.stats.pages_succeeded}ê°œ")
        logger.info(f"  ì‹¤íŒ¨í•œ í˜ì´ì§€: {self.stats.pages_failed}ê°œ")
        logger.info(f"  ê±´ë„ˆë›´ í˜ì´ì§€: {self.stats.pages_skipped_unchanged + self.stats.pages_skipped_by_kb + self.stats.pages_skipped_problematic}ê°œ")
        logger.info(f"  ë°œê²¬ëœ ë§í¬: {self.stats.total_links_found}ê°œ")
        logger.info(f"  í‰ê·  ì²˜ë¦¬ ì†ë„: {self.stats.avg_speed:.2f} í˜ì´ì§€/ì´ˆ")
        logger.info(f"  ì„±ê³µë¥ : {self.stats.success_rate():.1f}%")
        
        # ì„±ëŠ¥ í†µê³„ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
        stats_file = self.output_dir / "crawling_stats.json"
        try:
            with open(stats_file, 'w', encoding='utf-8') as f:
                json.dump(stats_dict, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"í†µê³„ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")

# ==========================================
# ê¸°ì¡´ FastCrawlerAgent í˜¸í™˜ ë˜í¼
# ==========================================

class SuperFastCrawlerAgent:
    """
    ê¸°ì¡´ FastCrawlerAgentì™€ ì™„ì „ í˜¸í™˜ë˜ëŠ” ê³ ì„±ëŠ¥ ë˜í¼
    ê¸°ì¡´ ì½”ë“œ ìˆ˜ì • ì—†ì´ ì¦‰ì‹œ ì„±ëŠ¥ í–¥ìƒ ì ìš© ê°€ëŠ¥
    """
    
    def __init__(self, config: Dict):
        # ê³ ì„±ëŠ¥ ì—ì´ì „íŠ¸ë¡œ ë‚´ë¶€ êµ¬í˜„
        self.hp_agent = HighPerformanceCrawlerAgent(config)
        
        # ê¸°ì¡´ ì¸í„°í˜ì´ìŠ¤ í˜¸í™˜ì„ ìœ„í•œ ì†ì„±ë“¤
        self.config = config
        self.site_identifier = config["site_identifier"]
        self.base_url = config["base_url"]
        
    async def run(self):
        """ê¸°ì¡´ run() ë©”ì„œë“œì™€ ë™ì¼í•œ ì¸í„°í˜ì´ìŠ¤"""
        return await self.hp_agent.run()
    
    def log_performance(self):
        """ê¸°ì¡´ log_performance() ë©”ì„œë“œ í˜¸í™˜"""
        self.hp_agent._log_final_statistics()

# ==========================================
# ë©”ì¸ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ ì—…ë°ì´íŠ¸
# ==========================================

async def run_high_performance_crawlers():
    """
    ê³ ì„±ëŠ¥ í¬ë¡¤ëŸ¬ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
    ê¸°ì¡´ run_crawlers.pyë¥¼ ëŒ€ì²´í•˜ëŠ” ê³ ì„±ëŠ¥ ë²„ì „
    """
    import sys
    sys.path.append('.')
    
    from src.config import load_configs_from_prompt_xlsx
    
    logging.basicConfig(
        level=logging.INFO, 
        format='%(asctime)s - %(levelname)s - [HP-CRAWLER] %(message)s'
    )
    logger = logging.getLogger(__name__)

    # Excel íŒŒì¼ì—ì„œ ì„¤ì • ë¡œë“œ
    configs = load_configs_from_prompt_xlsx()
    if not configs:
        logger.error("ì²˜ë¦¬í•  ì‚¬ì´íŠ¸ ì„¤ì •ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    logger.info(f"ğŸš€ ê³ ì„±ëŠ¥ í¬ë¡¤ëŸ¬ ì‹œì‘: {len(configs)}ê°œ ì‚¬ì´íŠ¸ ëŒ€ìƒ")
    logger.info("ğŸ“Š ì˜ˆìƒ ì²˜ë¦¬ ëŠ¥ë ¥: 2000+ í˜ì´ì§€/ì‚¬ì´íŠ¸, 5-15 í˜ì´ì§€/ì´ˆ")
    
    total_start_time = time.time()
    
    # ì‚¬ì´íŠ¸ë³„ ìˆœì°¨ ì²˜ë¦¬ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ìœ„í•´)
    # í•„ìš”ì‹œ ë³‘ë ¬ ì²˜ë¦¬ë¡œ ë³€ê²½ ê°€ëŠ¥
    for i, config in enumerate(configs, 1):
        site_start_time = time.time()
        logger.info(f"[{i}/{len(configs)}] {config['site_name']} ì²˜ë¦¬ ì‹œì‘...")
        
        agent = HighPerformanceCrawlerAgent(config)
        try:
            await agent.run()
        except Exception as e:
            logger.error(f"ì‚¬ì´íŠ¸ {config['site_name']} ì²˜ë¦¬ ì‹¤íŒ¨: {e}", exc_info=True)
        
        site_time = time.time() - site_start_time
        logger.info(f"[{i}/{len(configs)}] {config['site_name']} ì™„ë£Œ ({site_time:.1f}ì´ˆ)")
    
    total_time = time.time() - total_start_time
    logger.info(f"ğŸ‰ ì „ì²´ í¬ë¡¤ë§ ì™„ë£Œ! ì´ ì†Œìš” ì‹œê°„: {total_time/60:.1f}ë¶„")

# ==========================================
# ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ë° ë²¤ì¹˜ë§ˆí¬
# ==========================================

async def benchmark_performance():
    """ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸"""
    logger.info("ğŸ§ª ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘...")
    
    # í…ŒìŠ¤íŠ¸ ì„¤ì •
    test_config = {
        "site_identifier": "performance_test",
        "site_name": "ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‚¬ì´íŠ¸",
        "base_url": "https://example.com",
        "instruction_prompt": "ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ìš© í¬ë¡¤ë§",
        "max_pages_to_crawl": 500,  # í…ŒìŠ¤íŠ¸ìš© 500í˜ì´ì§€
        "crawl_delay": 0.1
    }
    
    # ê¸°ì¡´ ë²„ì „ê³¼ ê³ ì„±ëŠ¥ ë²„ì „ ë¹„êµ
    logger.info("1ï¸âƒ£ ê³ ì„±ëŠ¥ ë²„ì „ í…ŒìŠ¤íŠ¸...")
    hp_start = time.time()
    
    hp_agent = HighPerformanceCrawlerAgent(test_config)
    await hp_agent.run()
    
    hp_time = time.time() - hp_start
    hp_stats = hp_agent.stats
    
    logger.info(f"ê³ ì„±ëŠ¥ ë²„ì „ ê²°ê³¼:")
    logger.info(f"  ì²˜ë¦¬ ì‹œê°„: {hp_time:.2f}ì´ˆ")
    logger.info(f"  ì²˜ë¦¬ ì†ë„: {hp_stats.avg_speed:.2f} í˜ì´ì§€/ì´ˆ")
    logger.info(f"  ì„±ê³µë¥ : {hp_stats.success_rate():.1f}%")
    
    return {
        "hp_time": hp_time,
        "hp_speed": hp_stats.avg_speed,
        "hp_success_rate": hp_stats.success_rate()
    }

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == "benchmark":
        # ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
        asyncio.run(benchmark_performance())
    else:
        # ì¼ë°˜ í¬ë¡¤ë§ ì‹¤í–‰
        asyncio.run(run_high_performance_crawlers())
