import logging
import asyncio
import aiohttp
from bs4 import BeautifulSoup, Tag
from urllib.parse import urljoin, urlparse, parse_qs, unquote
from pathlib import Path
import io
import tempfile
import subprocess
import json
import time
import re
from typing import Dict, List, Tuple, Callable, Optional

# --- ê° íŒŒì¼ íƒ€ì…ì„ ì²˜ë¦¬í•˜ê¸° ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ---
from pypdf import PdfReader
from docx import Document
import pandas as pd
from pptx import Presentation
import hwp5

logger = logging.getLogger(__name__)

class DataExtractor:
    """
    [v2.6] 'ë¶„ì„'ê³¼ 'ì œê±°' ë¡œì§ì„ ëª…í™•íˆ ë¶„ë¦¬í•˜ì—¬ 'NoneType' ì˜¤ë¥˜ë¥¼
    ê·¼ë³¸ì ìœ¼ë¡œ í•´ê²°í•œ ìµœì¢… ì•ˆì •í™” ë²„ì „.
    """
    def __init__(self):
        timeout_config = aiohttp.ClientTimeout(total=120, connect=30, sock_read=90)
        self._session = aiohttp.ClientSession(timeout=timeout_config)

        self.failed_attachments_dir = Path("failed_attachments")
        self.failed_attachments_dir.mkdir(exist_ok=True)

        self.file_parsers: Dict[str, Callable] = {
            '.pdf': self._parse_pdf, '.docx': self._parse_docx,
            '.xlsx': self._parse_excel, '.xls': self._parse_excel,
            '.pptx': self._parse_pptx, '.hwp': self._parse_hwp,
        }

    # --- 1. ì§€ëŠ¥í˜• ë³¸ë¬¸ ì¶”ì¶œ (ì•Œê³ ë¦¬ì¦˜ ì•ˆì •ì„± ê°•í™”) ---

    def _calculate_content_score(self, element: Tag) -> float:
        """ ìš”ì†Œì˜ 'ë‚´ìš© ë°€ë„' ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤. """
        if not isinstance(element, Tag) or element.name in ['script', 'style', 'a']:
            return 0

        text = element.get_text(strip=True)
        text_length = len(text)

        if text_length < 100:
            return 0

        link_text_length = sum(len(a.get_text(strip=True)) for a in element.find_all('a'))
        pure_text_length = text_length - link_text_length
        link_density = link_text_length / max(1, text_length)
        score = pure_text_length * (1 - link_density**2)

        tag_id = element.get('id', '').lower()
        tag_class = ' '.join(element.get('class', [])).lower()

        if any(keyword in tag_id or keyword in tag_class for keyword in ['content', 'article', 'post', 'body', 'main', 'view']):
            score *= 1.5

        return score

    def _get_best_candidate(self, soup: BeautifulSoup) -> Optional[Tag]:
        """ ê°€ì¥ ì•ˆì •ì ì¸ ë°©ì‹ìœ¼ë¡œ ë³¸ë¬¸ í›„ë³´ë¥¼ ì°¾ìŠµë‹ˆë‹¤. """
        best_candidate = None
        highest_score = 0

        for element in soup.find_all(['div', 'article', 'section']):
            score = self._calculate_content_score(element)
            if score > highest_score:
                highest_score = score
                best_candidate = element

        return best_candidate if best_candidate else (soup.body or soup)

    def _clean_html_and_extract_text(self, soup: BeautifulSoup) -> str:
        # 1ë‹¨ê³„: ë¶ˆí•„ìš”í•œ íƒœê·¸ ì¼ê´„ ì œê±°
        for tag in soup(['script', 'style', 'header', 'footer', 'nav', 'aside', 'form', 'button', 'iframe', 'figure']):
            tag.decompose()

        # 2ë‹¨ê³„: ìµœê³  ì ìˆ˜ ì˜ì—­ ì„ ì •
        best_candidate = self._get_best_candidate(soup)
        if not best_candidate: return ""

        # [ìµœì¢… ìˆ˜ì •] 3ë‹¨ê³„: 'ë¶„ì„'ê³¼ 'ì œê±°'ë¥¼ ë¶„ë¦¬í•˜ì—¬ ì•ˆì •ì„± í™•ë³´
        # 3-1. ë¨¼ì € ì œê±°í•  ë…¸ì´ì¦ˆ íƒœê·¸ ëª©ë¡ì„ ëª¨ë‘ ì°¾ìŠµë‹ˆë‹¤.
        tags_to_decompose = []
        for tag in best_candidate.find_all(['div', 'section', 'ul', 'li', 'table']):
            # ìˆœíšŒ ì¤‘ Noneì´ ë˜ëŠ” ê²½ìš°ë¥¼ ì›ì²œ ì°¨ë‹¨
            if not isinstance(tag, Tag): continue

            tag_id = tag.get('id', '').lower()
            tag_class = ' '.join(tag.get('class', [])).lower()
            if any(keyword in tag_id or keyword in tag_class for keyword in ['comment', 'related', 'share', 'social', 'extra', 'footer', 'header', 'nav', 'menu', 'button', 'author', 'widget']):
                tags_to_decompose.append(tag)

        # 3-2. ë¶„ì„ì´ ëª¨ë‘ ëë‚œ í›„, ë³„ë„ì˜ ë£¨í”„ì—ì„œ ì•ˆì „í•˜ê²Œ ì œê±°í•©ë‹ˆë‹¤.
        for tag in tags_to_decompose:
            tag.decompose()

        raw_text = best_candidate.get_text(separator='\n', strip=False)
        return self._final_text_clean(raw_text)

    def _final_text_clean(self, text: str) -> str:
        text = re.sub(r'(\n\s*){3,}', '\n\n', text)
        lines = text.split('\n')
        cleaned_lines = []
        noise_keywords = [
            'ë‹¤ìš´ë¡œë“œ', 'ë·°ì–´', 'ì²¨ë¶€íŒŒì¼', 'ëª©ë¡ìœ¼ë¡œ', 'ì´ì „ê¸€', 'ë‹¤ìŒê¸€', 'ë§¨ìœ„ë¡œ',
            'Copyright', 'All rights reserved', 'ì°¾ì•„ì˜¤ì‹œëŠ” ê¸¸', 'ê°œì¸ì •ë³´ì²˜ë¦¬ë°©ì¹¨', 'ìœ ìš©í•œ ì •ë³´ê°€ ë˜ì—ˆë‚˜ìš”?'
        ]
        for line in lines:
            stripped_line = line.strip()
            if not stripped_line or len(stripped_line) < 10: continue
            if any(keyword in stripped_line for keyword in noise_keywords): continue
            if re.match(r'^\s*(ì‘ì„±ì|ë“±ë¡ì¼|ì¡°íšŒìˆ˜|ë‹´ë‹¹ë¶€ì„œ|í‚¤ì›Œë“œ|ë¶„ë¥˜)\s*[:\s]', stripped_line): continue
            cleaned_lines.append(stripped_line)
        return "\n".join(cleaned_lines)

    # --- 2. ì²¨ë¶€ íŒŒì¼ ì²˜ë¦¬ ---
    def _parse_pdf(self, content: bytes) -> str:
        return "\n".join([page.extract_text() for page in PdfReader(io.BytesIO(content)).pages if page.extract_text()])
    def _parse_docx(self, content: bytes) -> str:
        return "\n".join([para.text for para in Document(io.BytesIO(content)).paragraphs if para.text])
    def _parse_excel(self, content: bytes) -> str:
        xls = pd.ExcelFile(io.BytesIO(content))
        return "\n\n".join([xls.parse(sheet_name).to_string() for sheet_name in xls.sheet_names])
    def _parse_pptx(self, content: bytes) -> str:
        prs = Presentation(io.BytesIO(content))
        return "\n".join([shape.text for slide in prs.slides for shape in slide.shapes if hasattr(shape, "text")])
    def _parse_hwp(self, content: bytes) -> str:
        try:
            hwp = hwp5.HWPFile(io.BytesIO(content))
            text = hwp.body.text()
            if not text: raise ValueError("Extracted HWP text is empty.")
            return text
        except Exception:
            raise

    async def _save_failed_attachment(self, url: str, content_bytes: bytes, reason: str, site_identifier: str):
        try:
            site_dir = self.failed_attachments_dir / site_identifier
            site_dir.mkdir(exist_ok=True)
            timestamp = int(time.time())
            original_filename = Path(urlparse(url).path).name or f"{timestamp}.attachment"
            file_path = site_dir / f"{timestamp}_{original_filename}"
            with open(file_path, 'wb') as f: f.write(content_bytes)
            meta_path = site_dir / f"{timestamp}_{original_filename}.meta.json"
            meta_data = {
                "original_url": url, "saved_path": str(file_path), "failure_reason": reason,
                "timestamp": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime(timestamp))
            }
            with open(meta_path, 'w', encoding='utf-8') as f: json.dump(meta_data, f, ensure_ascii=False, indent=2)
            logger.warning(f"ğŸš¨ íŒŒì‹± ì‹¤íŒ¨! ì›ë³¸ íŒŒì¼ ë° ë©”íƒ€ë°ì´í„° ì €ì¥ ì™„ë£Œ: {file_path}")
        except Exception as e:
            logger.error(f"ì‹¤íŒ¨í•œ ì²¨ë¶€ íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    async def _extract_text_from_attachment(self, url: str, site_identifier: str) -> str:
        file_ext = Path(urlparse(url).path).suffix.lower()
        parser = self.file_parsers.get(file_ext)
        if not parser: return ""
        content_bytes = None
        try:
            async with self._session.get(url, timeout=60) as response:
                if response.status != 200: return ""
                content_bytes = await response.read()
            text = await asyncio.to_thread(parser, content_bytes)
            if not text: raise ValueError("íŒŒì„œê°€ ë¹ˆ í…ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤.")
            logger.info(f"âœ… {file_ext.upper()} í…ìŠ¤íŠ¸ ì¶”ì¶œ ì„±ê³µ: {url} ({len(text)}ì)")
            return f"\n\n--- [ì²¨ë¶€ íŒŒì¼ ì‹œì‘: {Path(url).name}] ---\n{text}\n--- [ì²¨ë¶€ íŒŒì¼ ë] ---\n"
        except Exception as e:
            logger.error(f"ì²¨ë¶€ íŒŒì¼({url}) ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            if content_bytes:
                await self._save_failed_attachment(url, content_bytes, str(e), site_identifier)
            return f"\n\n--- [ì²¨ë¶€ íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {Path(url).name}] ---\n"

    def _get_real_pdf_url_from_viewer(self, viewer_url: str, base_url: str) -> str | None:
        try:
            query_params = parse_qs(urlparse(viewer_url).query)
            if 'file' in query_params:
                return urljoin(base_url, unquote(query_params['file'][0]))
        except Exception: return None

    async def extract(self, url: str, base_url: str, site_identifier: str) -> Dict | None:
        try:
            async with self._session.get(url, timeout=30) as response:
                if response.status != 200 or 'text/html' not in response.headers.get('Content-Type', ''):
                    return None
                html = await response.text()

                soup_for_links = BeautifulSoup(html, 'html.parser')
                soup_for_text = BeautifulSoup(html, 'html.parser')

                title = soup_for_text.title.string.strip() if soup_for_text.title else url

                main_text = self._clean_html_and_extract_text(soup_for_text)

                all_links, attachment_links = [], []
                base_netloc = urlparse(base_url).netloc

                for a_tag in soup_for_links.find_all('a', href=True):
                    href = a_tag['href']
                    absolute_url = urljoin(base_url, href)

                    real_pdf_url = self._get_real_pdf_url_from_viewer(absolute_url, base_url)
                    if real_pdf_url and real_pdf_url not in attachment_links:
                        attachment_links.append(real_pdf_url); continue

                    file_ext = Path(urlparse(href).path).suffix.lower()
                    if file_ext in self.file_parsers and urlparse(absolute_url).netloc == base_netloc and absolute_url not in attachment_links:
                        attachment_links.append(absolute_url)
                    else:
                        all_links.append((absolute_url, a_tag.get_text(strip=True)))

                if attachment_links:
                    tasks = [self._extract_text_from_attachment(link, site_identifier) for link in attachment_links]
                    main_text += "".join(await asyncio.gather(*tasks))

                return {"url": url, "title": title, "main_text": main_text, "links": all_links}
        except Exception as e:
            logger.error(f"ë°ì´í„° ì¶”ì¶œ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {url}, {e}", exc_info=True)
            return None

    async def close_session(self):
        if self._session and not self._session.closed: await self._session.close()

