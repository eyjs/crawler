# src/crawler/data_extractor.py

import logging
import asyncio
import aiohttp
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse, parse_qs, unquote
from pathlib import Path
import io
import tempfile
import subprocess
import json
import time
import re
from typing import Dict, List, Tuple, Callable

# --- ê° íŒŒì¼ íƒ€ì…ì„ ì²˜ë¦¬í•˜ê¸° ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ---
from pypdf import PdfReader
from docx import Document
import pandas as pd
from pptx import Presentation
import hwp5

logger = logging.getLogger(__name__)

class DataExtractor:
    """
    HTML ë° ë‹¤ì–‘í•œ ì²¨ë¶€ íŒŒì¼ì—ì„œ ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ëŠ” ì§€ëŠ¥í˜• ì¶”ì¶œê¸°.
    ë³´ì¼ëŸ¬í”Œë ˆì´íŠ¸ ì œê±°, ë…¸ì´ì¦ˆ í•„í„°ë§, íŒŒì‹± ì‹¤íŒ¨ ê²©ë¦¬ ë¡œì§ì„ í¬í•¨í•œ ìµœì¢… ë²„ì „.
    """
    def __init__(self):
        self._session = aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=90))
        self.failed_attachments_dir = Path("failed_attachments")
        self.failed_attachments_dir.mkdir(exist_ok=True)

        self.file_parsers: Dict[str, Callable] = {
            '.pdf': self._parse_pdf, '.docx': self._parse_docx,
            '.xlsx': self._parse_excel, '.xls': self._parse_excel,
            '.pptx': self._parse_pptx, '.hwp': self._parse_hwp,
        }

    def _clean_text(self, text: str) -> str:
        """ì¶”ì¶œëœ í…ìŠ¤íŠ¸ì—ì„œ ì¼ë°˜ì ì¸ ë…¸ì´ì¦ˆ íŒ¨í„´ê³¼ ë¶ˆí•„ìš”í•œ ê³µë°±ì„ ì œê±°í•©ë‹ˆë‹¤."""
        # ì—¬ëŸ¬ ì¤„ë°”ê¿ˆì„ ìµœëŒ€ ë‘ ê°œë¡œ ì••ì¶•
        text = re.sub(r'(\n\s*){3,}', '\n\n', text)
        lines = text.split('\n')
        cleaned_lines = []

        noise_keywords = [
            'ë‹¤ìš´ë¡œë“œ', 'ë·°ì–´', 'ì²¨ë¶€íŒŒì¼', 'ëª©ë¡ìœ¼ë¡œ', 'ì´ì „ê¸€', 'ë‹¤ìŒê¸€', 'ë§¨ìœ„ë¡œ',
            'Copyright', 'All rights reserved', 'ì°¾ì•„ì˜¤ì‹œëŠ” ê¸¸', 'ê°œì¸ì •ë³´ì²˜ë¦¬ë°©ì¹¨'
        ]
        for line in lines:
            stripped_line = line.strip()
            if not stripped_line: continue
            if len(stripped_line) < 10 and not re.search('[ê°€-í£a-zA-Z0-9]', stripped_line): continue
            if any(keyword in stripped_line for keyword in noise_keywords): continue
            if re.match(r'^\s*(ì‘ì„±ì|ë“±ë¡ì¼|ì¡°íšŒìˆ˜|ë‹´ë‹¹ë¶€ì„œ|í‚¤ì›Œë“œ|ë¶„ë¥˜)\s*[:\s]', stripped_line): continue
            cleaned_lines.append(stripped_line)

        return "\n".join(cleaned_lines)

    # --- ê° íŒŒì¼ íƒ€ì…ë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë©”ì„œë“œ ---
    def _parse_pdf(self, content: bytes) -> str:
        return "\n".join([page.extract_text() for page in PdfReader(io.BytesIO(content)).pages if page.extract_text()])
    def _parse_docx(self, content: bytes) -> str:
        return "\n".join([para.text for para in Document(io.BytesIO(content)).paragraphs if para.text])
    def _parse_excel(self, content: bytes) -> str:
        xls = pd.ExcelFile(io.BytesIO(content))
        return "\n\n".join([xls.parse(sheet_name).to_string() for sheet_name in xls.sheet_names])
    def _parse_pptx(self, content: bytes) -> str:
        prs = Presentation(io.BytesIO(content))
        return "\n".join([shape.text for slide in prs.slides for shape in slide.shapes if hasattr(shape, "text")])
    def _parse_hwp(self, content: bytes) -> str:
        try:
            hwp = hwp5.HWPFile(io.BytesIO(content))
            text = hwp.body.text()
            if not text: raise ValueError("Extracted HWP text is empty.")
            return text
        except Exception as e:
            raise e # ì˜ˆì™¸ë¥¼ ë°œìƒì‹œì¼œ _extract_text_from_attachmentì—ì„œ ì²˜ë¦¬í•˜ë„ë¡ í•¨

    async def _save_failed_attachment(self, url: str, content_bytes: bytes, reason: str, site_identifier: str):
        try:
            site_dir = self.failed_attachments_dir / site_identifier
            site_dir.mkdir(exist_ok=True)
            timestamp = int(time.time())
            original_filename = Path(urlparse(url).path).name or f"{timestamp}.attachment"
            file_path = site_dir / f"{timestamp}_{original_filename}"
            with open(file_path, 'wb') as f: f.write(content_bytes)
            meta_path = site_dir / f"{timestamp}_{original_filename}.meta.json"
            meta_data = {
                "original_url": url, "saved_path": str(file_path), "failure_reason": reason,
                "timestamp": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime(timestamp))
            }
            with open(meta_path, 'w', encoding='utf-8') as f: json.dump(meta_data, f, ensure_ascii=False, indent=2)
            logger.warning(f"ğŸš¨ íŒŒì‹± ì‹¤íŒ¨! ì›ë³¸ íŒŒì¼ ë° ë©”íƒ€ë°ì´í„° ì €ì¥ ì™„ë£Œ: {file_path}")
        except Exception as e:
            logger.error(f"ì‹¤íŒ¨í•œ ì²¨ë¶€ íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    async def _extract_text_from_attachment(self, url: str, site_identifier: str) -> str:
        file_ext = Path(urlparse(url).path).suffix.lower()
        parser = self.file_parsers.get(file_ext)
        if not parser: return ""
        content_bytes = None
        try:
            async with self._session.get(url, timeout=60) as response:
                if response.status != 200: return ""
                content_bytes = await response.read()
            text = await asyncio.to_thread(parser, content_bytes)
            if not text: raise ValueError("íŒŒì„œê°€ ë¹ˆ í…ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤.")
            logger.info(f"âœ… {file_ext.upper()} í…ìŠ¤íŠ¸ ì¶”ì¶œ ì„±ê³µ: {url} ({len(text)}ì)")
            return f"\n\n--- ì²¨ë¶€ëœ {file_ext.upper()} íŒŒì¼ ë‚´ìš©: {url} ---\n{text}"
        except Exception as e:
            logger.error(f"ì²¨ë¶€ íŒŒì¼({url}) ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            if content_bytes:
                await self._save_failed_attachment(url, content_bytes, str(e), site_identifier)
            return f"\n\n--- ì²¨ë¶€ëœ {file_ext.upper()} íŒŒì¼ ë‚´ìš©: {url} ---"

    def _get_real_pdf_url_from_viewer(self, viewer_url: str, base_url: str) -> str | None:
        try:
            query_params = parse_qs(urlparse(viewer_url).query)
            if 'file' in query_params:
                return urljoin(base_url, unquote(query_params['file'][0]))
        except Exception: return None

    async def extract(self, url: str, base_url: str, site_identifier: str) -> Dict | None:
        try:
            async with self._session.get(url, timeout=30) as response:
                if response.status != 200 or 'text/html' not in response.headers.get('Content-Type', ''):
                    return None
                html = await response.text()
                soup = BeautifulSoup(html, 'html.parser')

                title = soup.title.string.strip() if soup.title else ""

                for tag in soup(['header', 'footer', 'nav', 'aside', 'script', 'style', 'form', 'button']):
                    tag.decompose()

                main_content = soup.find('main') or soup.find('article') or soup.find('div', id='content') or soup.find('div', class_='content') or soup.find('div', id='bodyContent') or soup.body

                raw_text = main_content.get_text(separator='\n', strip=False) if main_content else ""
                main_text = self._clean_text(raw_text)

                all_links, attachment_links = [], []
                base_netloc = urlparse(base_url).netloc

                for a_tag in soup.find_all('a', href=True):
                    href = a_tag['href']
                    absolute_url = urljoin(base_url, href)

                    real_pdf_url = self._get_real_pdf_url_from_viewer(absolute_url, base_url)
                    if real_pdf_url and real_pdf_url not in attachment_links:
                        attachment_links.append(real_pdf_url); continue

                    file_ext = Path(urlparse(href).path).suffix.lower()
                    if file_ext in self.file_parsers and urlparse(absolute_url).netloc == base_netloc and absolute_url not in attachment_links:
                        attachment_links.append(absolute_url)
                    else:
                        all_links.append((absolute_url, a_tag.get_text(strip=True)))

                if attachment_links:
                    tasks = [self._extract_text_from_attachment(link, site_identifier) for link in attachment_links]
                    main_text += "".join(await asyncio.gather(*tasks))

                return {"url": url, "title": title, "main_text": main_text, "links": all_links}
        except Exception as e:
            logger.error(f"ë°ì´í„° ì¶”ì¶œ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {url}, {e}", exc_info=True)
            return None

    async def close_session(self):
        if self._session and not self._session.closed: await self._session.close()