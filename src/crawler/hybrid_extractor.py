# src/crawler/hybrid_extractor.py

"""
Requests ê¸°ë°˜ ë¹„ë™ê¸° ìŠ¤íƒ€ì¼ í¬ë¡¤ëŸ¬
"""
import requests
import asyncio
import time
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from loguru import logger
from typing import Dict, List, Optional
from concurrent.futures import ThreadPoolExecutor

from config.settings import config # <-- ì„¤ì • ì„í¬íŠ¸

class AsyncStyleContentExtractor:
    """requestsë¥¼ ì‚¬ìš©í•˜ì§€ë§Œ ë¹„ë™ê¸° ìŠ¤íƒ€ì¼ë¡œ ë™ì‘í•˜ëŠ” í¬ë¡¤ëŸ¬"""

    def __init__(self, timeout: int = 30, delay: float = 1.0, max_workers: int = 5):
        self.timeout = timeout
        self.delay = delay
        self.max_workers = max_workers
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': config.user_agent # <-- ì„¤ì • ê°’ìœ¼ë¡œ ë³€ê²½
        })
        self.executor = ThreadPoolExecutor(max_workers=max_workers)

    async def __aenter__(self):
        """ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì§„ì…"""
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì¢…ë£Œ"""
        self.executor.shutdown(wait=True)
        self.session.close()

    def _fetch_sync(self, url: str) -> Dict[str, any]:
        """ë™ê¸°ì‹ í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸° (ë‚´ë¶€ ë©”ì„œë“œ)"""
        # logger.info(f"ğŸ” í˜ì´ì§€ ìš”ì²­: {url}") # Agentì—ì„œ ë¡œê·¸ë¥¼ ë‚¨ê¸°ë¯€ë¡œ ì¤‘ë³µ ë¡œê·¸ ì œê±°

        try:
            response = self.session.get(url, timeout=self.timeout)

            if response.status_code != 200:
                logger.warning(f"    ã„´ HTTP {response.status_code}: {url}")
                return {
                    'url': url,
                    'success': False,
                    'error': f'HTTP {response.status_code}',
                    'content': '',
                    'links': []
                }

            # ì»¨í…ì¸  ì •ë¦¬
            cleaned_content = self._clean_html_content(response.text)

            # ë§í¬ ì¶”ì¶œ
            links = self._extract_links(response.text, url)

            # logger.success(f"âœ… í˜ì´ì§€ ë¡œë“œ ì„±ê³µ: {url} (ì»¨í…ì¸  {len(cleaned_content)}ì, ë§í¬ {len(links)}ê°œ)")

            # ìš”ì²­ ê°„ê²© ì¤€ìˆ˜
            time.sleep(self.delay)

            return {
                'url': url,
                'success': True,
                'content': cleaned_content,
                'links': links,
                'content_length': len(cleaned_content),
                'links_count': len(links)
            }

        except requests.exceptions.Timeout:
            logger.error(f"    ã„´ â° íƒ€ì„ì•„ì›ƒ: {url}")
            return {
                'url': url,
                'success': False,
                'error': 'Timeout',
                'content': '',
                'links': []
            }
        except Exception as e:
            logger.error(f"    ã„´ âŒ í˜ì´ì§€ ë¡œë“œ ì‹¤íŒ¨ {url}: {e}")
            return {
                'url': url,
                'success': False,
                'error': str(e),
                'content': '',
                'links': []
            }

    async def fetch_page_content(self, url: str) -> Dict[str, any]:
        """ë¹„ë™ê¸° ìŠ¤íƒ€ì¼ í˜ì´ì§€ ì»¨í…ì¸  ê°€ì ¸ì˜¤ê¸°"""
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(self.executor, self._fetch_sync, url)

    def _clean_html_content(self, html: str) -> str:
        """HTMLì—ì„œ ì‹¤ì œ í…ìŠ¤íŠ¸ ì»¨í…ì¸ ë§Œ ì¶”ì¶œ"""
        try:
            soup = BeautifulSoup(html, 'html.parser')

            for tag in soup(['script', 'style', 'nav', 'header', 'footer',
                           'aside', 'menu', 'advertisement', 'ads']):
                tag.decompose()

            main_content = (
                soup.find('main') or
                soup.find('article') or
                soup.find('div', class_=['content', 'main', 'body']) or
                soup.body or
                soup
            )

            if main_content:
                text = main_content.get_text(separator=' ', strip=True)
                import re
                text = re.sub(r'\s+', ' ', text)
                return text

            return ""

        except Exception as e:
            logger.error(f"HTML ì •ë¦¬ ì‹¤íŒ¨: {e}")
            return ""

    def _extract_links(self, html: str, base_url: str) -> List[Dict[str, str]]:
        """í˜ì´ì§€ì—ì„œ ë§í¬ ì¶”ì¶œ"""
        try:
            soup = BeautifulSoup(html, 'html.parser')
            links = []

            for link_tag in soup.find_all('a', href=True):
                href = link_tag['href']
                text = link_tag.get_text(strip=True)

                if not text or len(text) < 2:
                    continue

                full_url = urljoin(base_url, href)

                parsed = urlparse(full_url)
                if parsed.scheme not in ['http', 'https']:
                    continue

                # í”„ë˜ê·¸ë¨¼íŠ¸ ì œê±° (#section)
                full_url = full_url.split('#')[0]

                context = self._get_link_context(link_tag)

                links.append({
                    'url': full_url,
                    'text': text[:100],
                    'context': context
                })

            return links

        except Exception as e:
            logger.error(f"ë§í¬ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []

    def _get_link_context(self, link_tag, context_length: int = 100) -> str:
        """ë§í¬ ì£¼ë³€ ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        try:
            parent = link_tag.parent
            if parent:
                context_text = parent.get_text(strip=True)
                return context_text[:context_length]
            return ""
        except:
            return ""